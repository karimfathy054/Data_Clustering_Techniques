{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQY7SZsR8t1_"
      },
      "source": [
        "|<h1 align=\"center\">Name</h1>|<h1 align=\"center\">ID</h1>|\n",
        "|---|---|\n",
        "|<h4 align=\"center\">Abd Allah Mohamed Abd Allah Mohamed Taman</h4>|<h4 align=\"center\">20010906</h4>|\n",
        "|<h4 align=\"center\">Karim Fathy Abd Alaziz Mohamed Mostafa</h4>|<h4 align=\"center\">20011116</h4>|\n",
        "|<h4 align=\"center\">Mahmoud Ali Ahmed Ali</h4>|<h4 align=\"center\">20011811</h4>|:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EL7U0iWZAfBF",
        "outputId": "85d597f0-c98e-4a99-f3c1-8acab0dae89c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZ7xySdt32ep"
      },
      "source": [
        "# <font color='red'>Download Dataset and Understand the Format</font>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y7WdJuczNfTe"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import f1_score\n",
        "import numpy as np\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.decomposition import PCA\n",
        "import pandas as pd\n",
        "from scipy.sparse.csgraph import connected_components\n",
        "from sklearn.metrics.pairwise import rbf_kernel\n",
        "from sklearn.metrics.cluster import contingency_matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jJ3zWluMPYXg"
      },
      "outputs": [],
      "source": [
        "def precision_cal(cluster_labels, true_labels):\n",
        "    unique_labels = np.unique(cluster_labels)\n",
        "    precision_scores = []\n",
        "    weights = []\n",
        "    for label in unique_labels:\n",
        "        cluster_indices = np.where(cluster_labels == label)[0]\n",
        "        ground_truth_cluster_labels = true_labels[cluster_indices]\n",
        "        # Majority class in the cluster\n",
        "        majority_class = np.argmax(np.bincount(ground_truth_cluster_labels))\n",
        "        # Predicted labels for the current cluster\n",
        "        predicted_labels = np.full_like(ground_truth_cluster_labels, majority_class)\n",
        "        # Compute precision for the current cluster\n",
        "        precision = precision_score(ground_truth_cluster_labels, predicted_labels, average='weighted', zero_division=1)\n",
        "        precision_scores.append(precision)\n",
        "    return np.sum(precision_scores)/len(precision_scores)\n",
        "\n",
        "def conditional_entropy_cal(cluster_labels, true_labels):\n",
        "    contingency = contingency_matrix(cluster_labels, true_labels)\n",
        "    contingency = np.asarray(contingency, dtype=np.float64)\n",
        "    contingency_sum = np.sum(contingency)\n",
        "    contingency /= contingency_sum  # convert contingency to probabilities\n",
        "    true_distribution = np.sum(contingency, axis=1)\n",
        "    cluster_distribution = np.sum(contingency, axis=0)\n",
        "    H_c_given_t = -np.sum(contingency * np.log(contingency.clip(min=1e-15)) / np.log(2))\n",
        "    H_t = -np.sum(true_distribution * np.log(true_distribution.clip(min=1e-15)) / np.log(2))\n",
        "\n",
        "    return H_c_given_t / H_t\n",
        "\n",
        "def recall_cal(cluster_labels, true_labels):\n",
        "    unique_labels = np.unique(cluster_labels)\n",
        "    recall_scores = []\n",
        "    for label in unique_labels:\n",
        "        cluster_indices = np.where(cluster_labels == label)[0]\n",
        "        ground_truth_cluster_labels = true_labels[cluster_indices]\n",
        "        majority_class = np.argmax(np.bincount(ground_truth_cluster_labels))\n",
        "        predicted_labels = np.full_like(ground_truth_cluster_labels, majority_class)\n",
        "        recall = recall_score(ground_truth_cluster_labels, predicted_labels, average='weighted')\n",
        "        recall_scores.append(recall)\n",
        "    return np.average(recall_scores)\n",
        "\n",
        "def f1_cal(cluster_labels, true_labels):\n",
        "    unique_labels = np.unique(cluster_labels)\n",
        "    f1_scores = []\n",
        "    for label in unique_labels:\n",
        "        cluster_indices = np.where(cluster_labels == label)[0]\n",
        "        ground_truth_cluster_labels = true_labels[cluster_indices]\n",
        "        majority_class = np.argmax(np.bincount(ground_truth_cluster_labels))\n",
        "        predicted_labels = np.full_like(ground_truth_cluster_labels, majority_class)\n",
        "        f1 = f1_score(ground_truth_cluster_labels, predicted_labels, average='weighted') # Change average to 'weighted'\n",
        "        f1_scores.append(f1)\n",
        "    return np.mean(f1_scores)\n",
        "\n",
        "def evaluate(true_labels, cluster_labels):\n",
        "    precision = precision_cal(cluster_labels, true_labels)\n",
        "    recall = recall_cal(cluster_labels, true_labels)\n",
        "    f1 = f1_cal(cluster_labels, true_labels)\n",
        "    conditional_entropy = conditional_entropy_cal(cluster_labels, true_labels)\n",
        "    return (precision, recall, f1, conditional_entropy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_100jsAi2p1"
      },
      "source": [
        "# <font color='red'>K-Means Algorithm => O(knd)</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_9YDgQvci1sH"
      },
      "outputs": [],
      "source": [
        "def kMeans_algo(data, k, epsilon, iterations):\n",
        "    n_rows = data.shape[0] #no of samples\n",
        "    random_idx = np.random.RandomState(42).permutation(n_rows)\n",
        "    centroids = data[random_idx[:k]] # generate random centroids\n",
        "    clusters = np.zeros(n_rows) # k clusters with data points in each cluster\n",
        "    proximityMatrix = np.zeros((n_rows, k)) # distance matrix from each sample to each centroid of k centroids to determine the nearest centroid for each data point\n",
        "    delta = np.inf # is the threshold we initiate it with infinity till it reaches to value <= epsilon\n",
        "    iteration = 0 # counter to count number of iterations\n",
        "    while delta > epsilon and iteration < iterations: # algorithm runs till max iterations reached or the total update in centoids <= epsilon\n",
        "        for i in range(k): # calculate distances from each data point to the k centroids\n",
        "            proximityMatrix[:, i] = np.linalg.norm(data - centroids[i], axis=1)\n",
        "\n",
        "        clusters = np.argmin(proximityMatrix, axis=1) # cluster assignment\n",
        "\n",
        "        iteration += 1\n",
        "        old_centroids = deepcopy(centroids) # store old centroids before update\n",
        "\n",
        "        for i in range(k): # centroids update\n",
        "            centroids[i, :] = np.mean(data[clusters == i, :], axis=0)\n",
        "            if np.isnan(centroids).any():\n",
        "                centroids[i] = old_centroids[i]\n",
        "\n",
        "        delta = np.linalg.norm(centroids - old_centroids) # update delta\n",
        "\n",
        "    return clusters, centroids # return the k clusters and centroids\n",
        "\n",
        "# this function is to predict cluster of the new data points given the training centroids and the data points\n",
        "def predict(centroids, data):\n",
        "    k = len(centroids)\n",
        "    n = data.shape[0]\n",
        "    proximityMatrix = np.zeros((n, k))\n",
        "    for i in range(k):\n",
        "        proximityMatrix[:, i] = np.linalg.norm(data - centroids[i], axis=1)\n",
        "    predicted_labels = np.argmin(proximityMatrix, axis=1)\n",
        "    return predicted_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwuqFl5s3k9r"
      },
      "source": [
        "# <font color='red'>K-Means evaluation.</font>\n",
        "<h3 style='color:red;'>In this section we show how kmeans algorithm behaves with <br>\n",
        "1- taking the mean of each column in each segment resulting in 45 features\n",
        "for each data point.<br>\n",
        "2- Flattening all the features together in 45 x 125 = 5625 features for each\n",
        "data point. For this solution, you are required to reduce the number of\n",
        "dimensions and work on the projected data after reducing the dimensions.</h3>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gEnbeIRY7Mv2"
      },
      "outputs": [],
      "source": [
        "def driver_code_kmeans(address):\n",
        "    D_train, D_test, ground_truth_train, ground_truth_test = read_data(address)\n",
        "    pca_dim = 45\n",
        "    for avg_or_pca in range(0, 2, 1):\n",
        "        print('solution: ', avg_or_pca + 1)\n",
        "        for K_clusters in [8, 13, 19, 28, 38]:\n",
        "            if avg_or_pca == 0:\n",
        "                D_training = reduce_dimensions(D_train, avg_or_pca, pca_dim)\n",
        "                clusters, centroids = kMeans_algo(np.array(D_training), K_clusters, 0.001, 200)\n",
        "                train_pred = predict(centroids, np.array(D_training))\n",
        "                kmeans_precision, kmeans_recall, kmeans_f1, kmeans_conditional_entropy = evaluate(\n",
        "                    ground_truth_train,\n",
        "                    train_pred)\n",
        "                print('train on k = ', K_clusters)\n",
        "                print(\n",
        "                    f'precision : {kmeans_precision}, recall: {kmeans_recall}, f1-measure: {kmeans_f1}, cond_entropy: {kmeans_conditional_entropy}, ')\n",
        "                D_testing = reduce_dimensions(D_test, avg_or_pca, pca_dim)\n",
        "                test_pred = predict(centroids, np.array(D_testing))\n",
        "                kmeans_precision, kmeans_recall, kmeans_f1, kmeans_conditional_entropy = evaluate(\n",
        "                    ground_truth_test,\n",
        "                    test_pred)\n",
        "                print('test on k = ', K_clusters)\n",
        "                print(\n",
        "                    f'precision : {kmeans_precision}, recall: {kmeans_recall}, f1-measure: {kmeans_f1}, cond_entropy: {kmeans_conditional_entropy}, ')\n",
        "            else:\n",
        "                for pca_dim in [45, 200, 600, 800, 100]:\n",
        "                    D_training = reduce_dimensions(D_train, avg_or_pca, pca_dim)\n",
        "                    clusters, centroids = kMeans_algo(np.array(D_training), K_clusters, 0.001, 200)\n",
        "                    train_pred = predict(centroids, np.array(D_training))\n",
        "                    kmeans_precision, kmeans_recall, kmeans_f1, kmeans_conditional_entropy = evaluate(\n",
        "                        ground_truth_train,\n",
        "                        train_pred)\n",
        "                    print('train on k = ', K_clusters)\n",
        "                    print(f'dimensions: {pca_dim} ' +\n",
        "                          f'precision : {kmeans_precision}, recall: {kmeans_recall}, f1-measure: {kmeans_f1}, cond_entropy: {kmeans_conditional_entropy}, ')\n",
        "                    D_testing = reduce_dimensions(D_test, avg_or_pca, pca_dim)\n",
        "                    test_pred = predict(centroids, np.array(D_testing))\n",
        "                    kmeans_precision, kmeans_recall, kmeans_f1, kmeans_conditional_entropy = evaluate(\n",
        "                        ground_truth_test,\n",
        "                        test_pred)\n",
        "                    print('test on k = ', K_clusters)\n",
        "                    print(f'dimensions: {pca_dim} ' +\n",
        "                          f'precision : {kmeans_precision}, recall: {kmeans_recall}, f1-measure: {kmeans_f1}, cond_entropy: {kmeans_conditional_entropy}, ')\n",
        "\n",
        "\n",
        "driver_code_kmeans(path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAaCTd3G5tA0"
      },
      "source": [
        "# <font color='lightgreen'>Some notes according to kmeans evaluation.</font>\n",
        "### <li>the results in the second solution is better than first one as in the first one we just take the mean of each column which summation of the features numbers may result in eliminating each other and don't add a value especially if the feature values is different and has a good variance</li>\n",
        "### <li>Second solution helps us to add a valuable feature values and thus the clustering is better.</li>\n",
        "### <li>in case of the second solution we have observed from the results of the evaluation that if we increase the dimensions taken in consideration, then we will obtain better results but this in majority but not in all cases, As there are cases we increased the dimensions taken for the data point but with results getting worse, this is because it may cause noise to the data more than it adds.</li>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0JKSM6nCD-m"
      },
      "source": [
        "# <font color='lightgreen'>Best K in the 1st solution.</font>\n",
        "### From our analysis, we can say that with k = 28 we get best results in test set clustering."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZ-esjV-C8pR"
      },
      "source": [
        "# <font color='lightgreen'>Best K and dimensions in 2nd solution.</font>\n",
        "### k = 8 and #dims = 600."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvozHwiLD0BX"
      },
      "source": [
        "<font color='violet'>\n",
        "<table>\n",
        "  <thead>\n",
        "    <tr>\n",
        "      <th colspan=\"2\" style=\"color: violet;\"><b>solution: 1</b></th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <td>train on k = 8</td>\n",
        "      <td>\n",
        "        <table>\n",
        "          <thead>\n",
        "            <tr>\n",
        "              <th>Metric</th>\n",
        "              <th>Value</th>\n",
        "            </tr>\n",
        "          </thead>\n",
        "          <tbody>\n",
        "            <tr>\n",
        "              <td>Precision</td>\n",
        "              <td>0.327988</td>\n",
        "            </tr>\n",
        "            <tr>\n",
        "              <td>Recall</td>\n",
        "              <td>0.879735</td>\n",
        "            </tr>\n",
        "            <tr>\n",
        "              <td>F1-Score</td>\n",
        "              <td>0.543788</td>\n",
        "            </tr>\n",
        "            <tr>\n",
        "              <td>Conditional Entropy</td>\n",
        "              <td>1.703264</td>\n",
        "            </tr>\n",
        "          </tbody>\n",
        "        </table>\n",
        "      </td>\n",
        "    </tr>\n",
        "    <!-- More rows for other metrics and values -->\n",
        "  </tbody>\n",
        "  <thead>\n",
        "    <tr>\n",
        "      <th colspan=\"2\" style=\"color: violet;\"><b>solution: 1</b></th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <td>train on k = 8</td>\n",
        "      <td>\n",
        "        <table>\n",
        "          <thead>\n",
        "            <tr>\n",
        "              <th>Metric</th>\n",
        "              <th>Value</th>\n",
        "            </tr>\n",
        "          </thead>\n",
        "          <tbody>\n",
        "            <tr>\n",
        "              <td>Precision</td>\n",
        "              <td>0.327988</td>\n",
        "            </tr>\n",
        "            <tr>\n",
        "              <td>Recall</td>\n",
        "              <td>0.879735</td>\n",
        "            </tr>\n",
        "            <tr>\n",
        "              <td>F1-Score</td>\n",
        "              <td>0.543788</td>\n",
        "            </tr>\n",
        "            <tr>\n",
        "              <td>Conditional Entropy</td>\n",
        "              <td>1.703264</td>\n",
        "            </tr>\n",
        "          </tbody>\n",
        "        </table>\n",
        "      </td>\n",
        "    </tr>\n",
        "    <!-- More rows for other metrics and values -->\n",
        "  </tbody>\n",
        "  <thead>\n",
        "    <tr>\n",
        "      <th colspan=\"2\" style=\"color: violet;\"><b>solution: 1</b></th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <td>train on k = 8</td>\n",
        "      <td>\n",
        "        <table>\n",
        "          <thead>\n",
        "            <tr>\n",
        "              <th>Metric</th>\n",
        "              <th>Value</th>\n",
        "            </tr>\n",
        "          </thead>\n",
        "          <tbody>\n",
        "            <tr>\n",
        "              <td>Precision</td>\n",
        "              <td>0.327988</td>\n",
        "            </tr>\n",
        "            <tr>\n",
        "              <td>Recall</td>\n",
        "              <td>0.879735</td>\n",
        "            </tr>\n",
        "            <tr>\n",
        "              <td>F1-Score</td>\n",
        "              <td>0.543788</td>\n",
        "            </tr>\n",
        "            <tr>\n",
        "              <td>Conditional Entropy</td>\n",
        "              <td>1.703264</td>\n",
        "            </tr>\n",
        "          </tbody>\n",
        "        </table>\n",
        "      </td>\n",
        "    </tr>\n",
        "    <!-- More rows for other metrics and values -->\n",
        "  </tbody>\n",
        "  <thead>\n",
        "    <tr>\n",
        "      <th colspan=\"2\" style=\"color: violet;\"><b>solution: 1</b></th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <td>train on k = 8</td>\n",
        "      <td>\n",
        "        <table>\n",
        "          <thead>\n",
        "            <tr>\n",
        "              <th>Metric</th>\n",
        "              <th>Value</th>\n",
        "            </tr>\n",
        "          </thead>\n",
        "          <tbody>\n",
        "            <tr>\n",
        "              <td>Precision</td>\n",
        "              <td>0.327988</td>\n",
        "            </tr>\n",
        "            <tr>\n",
        "              <td>Recall</td>\n",
        "              <td>0.879735</td>\n",
        "            </tr>\n",
        "            <tr>\n",
        "              <td>F1-Score</td>\n",
        "              <td>0.543788</td>\n",
        "            </tr>\n",
        "            <tr>\n",
        "              <td>Conditional Entropy</td>\n",
        "              <td>1.703264</td>\n",
        "            </tr>\n",
        "          </tbody>\n",
        "        </table>\n",
        "      </td>\n",
        "    </tr>\n",
        "    <!-- More rows for other metrics and values -->\n",
        "  </tbody>\n",
        "  <thead>\n",
        "    <tr>\n",
        "      <th colspan=\"2\" style=\"color: violet;\"><b>solution: 1</b></th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <td>train on k = 8</td>\n",
        "      <td>\n",
        "        <table>\n",
        "          <thead>\n",
        "            <tr>\n",
        "              <th>Metric</th>\n",
        "              <th>Value</th>\n",
        "            </tr>\n",
        "          </thead>\n",
        "          <tbody>\n",
        "            <tr>\n",
        "              <td>Precision</td>\n",
        "              <td>0.327988</td>\n",
        "            </tr>\n",
        "            <tr>\n",
        "              <td>Recall</td>\n",
        "              <td>0.879735</td>\n",
        "            </tr>\n",
        "            <tr>\n",
        "              <td>F1-Score</td>\n",
        "              <td>0.543788</td>\n",
        "            </tr>\n",
        "            <tr>\n",
        "              <td>Conditional Entropy</td>\n",
        "              <td>1.703264</td>\n",
        "            </tr>\n",
        "          </tbody>\n",
        "        </table>\n",
        "      </td>\n",
        "    </tr>\n",
        "    <!-- More rows for other metrics and values -->\n",
        "  </tbody>\n",
        "</table>\n",
        "</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGOT65E4OeAZ"
      },
      "outputs": [],
      "source": [
        "def read_data(address):\n",
        "    D_train = []\n",
        "    D_test = []\n",
        "    ground_truth_train = []\n",
        "    ground_truth_test = []\n",
        "    for activity in range(1, 20):\n",
        "        for person in range(1, 9):\n",
        "            for segment in range(1, 61):\n",
        "                a = str(activity)\n",
        "                p = str(person)\n",
        "                s = str(segment)\n",
        "                if(activity < 10):\n",
        "                    a = \"0\" + a\n",
        "                if(segment < 10):\n",
        "                    s = \"0\" + s\n",
        "                p = f'a{a}/p{p}/s{s}.txt'\n",
        "                data_point = pd.read_csv(address+p, header=None)\n",
        "                if(segment < 49):\n",
        "                    D_train.append(data_point)\n",
        "                    ground_truth_train.append(activity)\n",
        "                else:\n",
        "                    D_test.append(data_point)\n",
        "                    ground_truth_test.append(activity)\n",
        "\n",
        "    D_train = np.array(D_train)\n",
        "    ground_truth_train = np.array(ground_truth_train)\n",
        "    D_test = np.array(D_test)\n",
        "    ground_truth_test = np.array(ground_truth_test)\n",
        "    return D_train, D_test, ground_truth_train, ground_truth_test\n",
        "def average_on_columns(D):\n",
        "    new_D = []\n",
        "    for i in range(len(D)):\n",
        "        avg_data_point = np.mean(D[i], axis=0)\n",
        "        new_D.append(avg_data_point)\n",
        "    return new_D\n",
        "def flatten(D):\n",
        "    new_D = []\n",
        "    for i in range(len(D)):\n",
        "        L = []\n",
        "        for j in range(len(D[i])):\n",
        "            for k in range(len(D[i][j])):\n",
        "                L.append(D[i][j][k])\n",
        "        new_D.append(L)\n",
        "    return new_D\n",
        "def pca_performer(D, pca_dim):\n",
        "    pca = PCA(n_components = pca_dim)\n",
        "    return pca.fit_transform(flatten(D))\n",
        "def reduce_dimensions(D, avg_or_pca, pca_dim):\n",
        "    if avg_or_pca == 0:\n",
        "        return average_on_columns(D)\n",
        "    else:\n",
        "        return pca_performer(D, pca_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tV9k3kGhOn08"
      },
      "outputs": [],
      "source": [
        "def compute_rbf(affinity_matrix, q, gamma):\n",
        "    rbf_sim = rbf_kernel(affinity_matrix, gamma=gamma)\n",
        "    adjacency_matrix = np.zeros_like(rbf_sim)\n",
        "    for i, row in enumerate(rbf_sim):\n",
        "        nearest_neighbors_indices = np.argsort(row)[::-1]\n",
        "        nearest_neighbors_indices = nearest_neighbors_indices[:q]\n",
        "        for j in nearest_neighbors_indices:\n",
        "            if i in np.argsort(rbf_sim[j])[::-1][:q]:\n",
        "                adjacency_matrix[i, j] = 1\n",
        "                adjacency_matrix[j, i] = 1\n",
        "\n",
        "    n_components, labels = connected_components(adjacency_matrix)\n",
        "    return (adjacency_matrix, n_components)\n",
        "\n",
        "def construct_delta_matrix(A):\n",
        "    delta_matrix = np.zeros((len(A), len(A)))\n",
        "    for i in range(len(A)):\n",
        "        row_sum = np.sum(A[i])\n",
        "        delta_matrix[i, i] = row_sum\n",
        "    return delta_matrix\n",
        "\n",
        "def compute_lrw(L, delta):\n",
        "    Lrw = np.zeros((len(L), len(L[0])))\n",
        "    for i in range(len(L)):\n",
        "        for j in range(len(L[0])):\n",
        "            if delta[i][i] == 0:\n",
        "                Lrw[i][j] = 0\n",
        "            else:\n",
        "                Lrw[i][j] = L[i][j] / delta[i][i]\n",
        "    return Lrw\n",
        "\n",
        "def getY(U):\n",
        "   # Y = normalize(U, norm='l2', axis=1)\n",
        "    return U\n",
        "\n",
        "def get_model(Y, k):\n",
        "    kmeans = KMeans(n_clusters=k, n_init=10)\n",
        "    kmeans.fit_predict(Y)\n",
        "    return kmeans\n",
        "\n",
        "\n",
        "def spectral_cluster(D, gamma, nearest_n_after_rbf, K_clusters, ground_truth):\n",
        "    (A, n_component) = compute_rbf(D, nearest_n_after_rbf, gamma)\n",
        "    delta = construct_delta_matrix(A)\n",
        "    L = delta - A\n",
        "    Lrw = compute_lrw(L, delta)\n",
        "    eigval, eigvecs = np.linalg.eig(Lrw)\n",
        " #   sorted_indices = np.argsort(eigval)\n",
        "  #  eigval = eigval[sorted_indices]\n",
        "   # eigvecs = eigvecs[:, sorted_indices]\n",
        "    model = get_model(getY(np.real(eigvecs[:, n_component:n_component+K_clusters])), K_clusters)\n",
        "    precision, recall, f1, conditional_entropy = evaluate(ground_truth, model.labels_)\n",
        "    return precision, recall, f1, conditional_entropy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tjz_oCU0z1o6"
      },
      "outputs": [],
      "source": [
        "def conditional_entropy(true_labels,predictions):\n",
        "    n_samples = len(true_labels)\n",
        "    true_labels = np.array(true_labels)\n",
        "    predictions = np.array(predictions)\n",
        "    clusters={}\n",
        "    cluster_ids = np.unique(predictions)\n",
        "    for id in cluster_ids:\n",
        "        cluster = true_labels[predictions == id]\n",
        "        clusters[id]=cluster\n",
        "    true_clusters_ids = np.unique(true_labels)\n",
        "    entropy = 0.0\n",
        "    for i,cluster in enumerate(clusters.values()):\n",
        "        cluster = np.array(cluster)\n",
        "        cluster_entropy = 0.0\n",
        "        for label in  true_clusters_ids:\n",
        "            count = np.where(cluster == label)[0].size\n",
        "            if(count!=0):\n",
        "                cluster_entropy -= (count/cluster.size)*np.log(count/cluster.size)\n",
        "        entropy += (cluster.size/n_samples) * cluster_entropy\n",
        "    return entropy\n",
        "def percision_recall_f1_score(true_labels,predictions):\n",
        "  n_samples = len(true_labels)\n",
        "  true_labels = np.array(true_labels)\n",
        "  t_labels,true_counts = np.unique(true_labels,return_counts=True)\n",
        "  predictions = np.array(predictions)\n",
        "  clusters={}\n",
        "  cluster_ids = np.unique(predictions)\n",
        "  for id in cluster_ids:\n",
        "    cluster = true_labels[predictions == id]\n",
        "    clusters[id]=cluster\n",
        "  percision = 0.0\n",
        "  recall = 0.0\n",
        "  f1_scores = []\n",
        "  flag=True\n",
        "  i=0\n",
        "\n",
        "  for cluster in clusters.values():\n",
        "    elements_classes,counts = np.unique(cluster,return_counts=True)\n",
        "    class_purity = (np.max(counts)/cluster.size)\n",
        "    percision+=(np.max(counts)/n_samples)\n",
        "    max_element = elements_classes[np.argmax(counts)]\n",
        "    class_recall  = np.max(counts)/true_counts[t_labels==max_element][0]\n",
        "    recall+=class_recall*(cluster.size/n_samples)\n",
        "    Fi = (2*class_purity*class_recall)/(class_purity+class_recall)\n",
        "    f1_scores.append(Fi)\n",
        "  return percision,recall,np.mean(f1_scores)\n",
        "\n",
        "def evaluate(true_labels, cluster_labels):\n",
        "    percision,recall,f1 = percision_recall_f1_score(true_labels,cluster_labels)\n",
        "    conditional_entroy = conditional_entropy(true_labels, cluster_labels)\n",
        "    return (percision, recall, f1, conditional_entroy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ya8d0qlWPaHD"
      },
      "outputs": [],
      "source": [
        "def driver_code(address):\n",
        "    D_train, D_test, ground_truth_train, ground_truth_test = read_data(address)\n",
        "    D = []\n",
        "    ground_truth = []\n",
        "    for avg_or_pca in range(0, 2):\n",
        "      for test_or_train in range(0, 2): # now we train only\n",
        "       for pca_dim in [800]:\n",
        "        if test_or_train == 0:\n",
        "            D = reduce_dimensions(D_train, avg_or_pca, pca_dim)\n",
        "            ground_truth = ground_truth_train\n",
        "        else:\n",
        "            D = reduce_dimensions(D_test, avg_or_pca, pca_dim)\n",
        "            ground_truth = ground_truth_test\n",
        "        for K_clusters in [19]:\n",
        "            for gamma in [.1, .4, 1, 2]:\n",
        "                for nearest_n_after_rbf in [20, 100, 400, 800]:\n",
        "                    spec_precision, spec_recall, spec_f1, spec_conditional_entropy = spectral_cluster(D, gamma, nearest_n_after_rbf,\n",
        "                                                                                    K_clusters, ground_truth)\n",
        "                    print(f'{test_or_train}, {avg_or_pca}, {pca_dim}, {K_clusters}, {gamma}, {nearest_n_after_rbf}, '+\n",
        "                         f'{spec_precision}, {spec_recall}, {spec_f1}, {spec_conditional_entropy}')\n",
        "\n",
        "\n",
        "driver_code('/content/drive/MyDrive/data/')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3a4Ba3wkloIC"
      },
      "source": [
        "# Summary of Spectral Clustering\n",
        "We've tried so many functions to compute the matrix A, here there are:\n",
        "``` python\n",
        "import numpy as np\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from scipy.sparse.csgraph import connected_components\n",
        "\n",
        "def construct_adjacency_matrix(affinity_matrix, initial_q=15, increment=15):\n",
        "    num_points = len(affinity_matrix)\n",
        "    adjacency_matrix = np.zeros((num_points, num_points))\n",
        "    cosine_sim = cosine_similarity(affinity_matrix)\n",
        "\n",
        "    # Starting with initial_q\n",
        "    q = initial_q\n",
        "\n",
        "    while True:\n",
        "        # Find the q nearest neighbors for each data point\n",
        "        nbrs = NearestNeighbors(n_neighbors=q+1, metric='cosine').fit(affinity_matrix)\n",
        "        distances, indices = nbrs.kneighbors(affinity_matrix)\n",
        "\n",
        "        # Connect mutually nearest neighbors\n",
        "        for i in range(num_points):\n",
        "            for j in indices[i]:\n",
        "                if i != j and i in indices[j]:\n",
        "                    adjacency_matrix[i, j] = cosine_sim[i][j]\n",
        "                    adjacency_matrix[j, i] = cosine_sim[i][j]\n",
        "\n",
        "        # Check if the graph is connected\n",
        "        n_components, labels = connected_components(adjacency_matrix)\n",
        "        if np.max(labels) == 0:\n",
        "            break  # Graph is connected, exit the loop\n",
        "        print(\"need increase\")\n",
        "        print(n_components)\n",
        "        # Increase q and reset adjacency_matrix\n",
        "        q += increment\n",
        "        adjacency_matrix = np.zeros((num_points, num_points))\n",
        "\n",
        "    return adjacency_matrix\n",
        "```\n",
        "\n",
        "``` python\n",
        "import numpy as np\n",
        "from scipy.sparse.csgraph import connected_components\n",
        "def min_positive_number(arr):\n",
        "    positives = arr[arr > 0]    \n",
        "    if positives.size == 0:\n",
        "        return None  \n",
        "    min_positive = np.min(positives)\n",
        "    return min_positive\n",
        "def cos_plus(affinity_matrix, threshold):\n",
        "    num_points = len(affinity_matrix)\n",
        "    cosine_sim = cosine_similarity(affinity_matrix)\n",
        "    min_num_not_equal_zero = min_positive_number(cosine_sim)\n",
        "    cosine_sim[cosine_sim < threshold] = 0\n",
        "    cosine_sim[cosine_sim > threshold] = 1\n",
        "    n_components, labels = connected_components(cosine_sim)\n",
        "    np.savetxt('j:/row.txt', [np.max(labels), min_num_not_equal_zero])\n",
        "    return cosine_sim\n",
        "```\n",
        "```python\n",
        "import numpy as np\n",
        "from scipy.sparse.csgraph import connected_components\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def cos_plus_plus(affinity_matrix, q):\n",
        "    cosine_sim = cosine_similarity(affinity_matrix)\n",
        "    adjacency_matrix = np.zeros_like(cosine_sim)\n",
        "    for i, row in enumerate(cosine_sim):\n",
        "        nearest_neighbors_indices = np.argsort(row)[::-1]\n",
        "        nearest_neighbors_indices = nearest_neighbors_indices[:q]\n",
        "        for j in nearest_neighbors_indices:\n",
        "            if i in np.argsort(cosine_sim[j])[::-1][:q]:\n",
        "                adjacency_matrix[i, j] = 1\n",
        "                adjacency_matrix[j, i] = 1\n",
        "\n",
        "    n_components, labels = connected_components(adjacency_matrix)\n",
        "    np.savetxt('j:/row.txt', [n_components])\n",
        "    return adjacency_matrix\n",
        "```\n",
        "```python\n",
        "import numpy as np\n",
        "from scipy.sparse.csgraph import connected_components\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def cos_plus_plus_plus(affinity_matrix, q):\n",
        "    cosine_sim = cosine_similarity(affinity_matrix)\n",
        "    for i, row in enumerate(cosine_sim):\n",
        "        nearest_neighbors_indices = np.argsort(row)[::-1]\n",
        "        nearest_neighbors_indices = nearest_neighbors_indices[:q]\n",
        "        for j in nearest_neighbors_indices:\n",
        "                cosine_sim[i, j] = 1\n",
        "                cosine_sim[j, i] = 1\n",
        "\n",
        "    n_components, labels = connected_components(cosine_sim)\n",
        "    np.savetxt('j:/row.txt', [n_components])\n",
        "    return cosine_sim\n",
        "\n",
        "```\n",
        "```python\n",
        "import numpy as np\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from scipy.sparse.csgraph import connected_components\n",
        "from sklearn.metrics.pairwise import rbf_kernel\n",
        "\n",
        "def rbf(affinity_matrix, initial_q, increment, gamma):\n",
        "    num_points = len(affinity_matrix)\n",
        "    adjacency_matrix = np.zeros((num_points, num_points))\n",
        "    \n",
        "    # Starting with initial_q\n",
        "    q = initial_q\n",
        "\n",
        "    while True:\n",
        "        # Compute the RBF kernel matrix\n",
        "        affinity_matrix_rbf = rbf_kernel(affinity_matrix, gamma=gamma)\n",
        "        # Find the q nearest neighbors for each data point based on the RBF kernel matrix\n",
        "        nbrs = NearestNeighbors(n_neighbors=q+1, algorithm='brute', metric='precomputed')\n",
        "        nbrs.fit(affinity_matrix_rbf)  # Fit the model with the RBF kernel matrix\n",
        "        distances, indices = nbrs.kneighbors(affinity_matrix_rbf)\n",
        "\n",
        "        # Connect mutually nearest neighbors\n",
        "        for i in range(num_points):\n",
        "            for j in indices[i]:\n",
        "                if i != j and i in indices[j]:\n",
        "                    adjacency_matrix[i, j] = affinity_matrix_rbf[i][j]\n",
        "                    adjacency_matrix[j, i] = affinity_matrix_rbf[i][j]\n",
        "\n",
        "        # Check if the graph is connected\n",
        "        n_components, labels = connected_components(adjacency_matrix)\n",
        "        if np.max(labels) == 0:\n",
        "            break  # Graph is connected, exit the loop\n",
        "\n",
        "        print(\"need increase\")\n",
        "        print(np.max(labels))        \n",
        "        # Increase q and reset adjacency_matrix\n",
        "        q += increment\n",
        "        adjacency_matrix = np.zeros((num_points, num_points))\n",
        "\n",
        "    return adjacency_matrix\n",
        "```\n",
        "```python\n",
        "import numpy as np\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from scipy.sparse.csgraph import connected_components\n",
        "\n",
        "def laplacian_kernel(X, sigma=1.0):\n",
        "    n_samples = X.shape[0]\n",
        "    K = np.zeros((n_samples, n_samples))\n",
        "    for i in range(n_samples):\n",
        "        for j in range(n_samples):\n",
        "            K[i, j] = np.exp(-np.linalg.norm(X[i] - X[j]) / sigma)\n",
        "    return K\n",
        "\n",
        "def rbf_laplacian(affinity_matrix, initial_q, increment, sigma):\n",
        "    num_points = len(affinity_matrix)\n",
        "    adjacency_matrix = np.zeros((num_points, num_points))\n",
        "    K = laplacian_kernel(affinity_matrix, sigma)\n",
        "\n",
        "    # Starting with initial_q\n",
        "    q = initial_q\n",
        "\n",
        "    while True:\n",
        "        # Find the q nearest neighbors for each data point\n",
        "        nbrs = NearestNeighbors(n_neighbors=q+1, metric='euclidean').fit(affinity_matrix)\n",
        "        distances, indices = nbrs.kneighbors(affinity_matrix)\n",
        "\n",
        "        # Connect mutually nearest neighbors\n",
        "        for i in range(num_points):\n",
        "            for j in indices[i]:\n",
        "                if i != j and i in indices[j]:\n",
        "                    adjacency_matrix[i, j] = K[i][j]\n",
        "                    adjacency_matrix[j, i] = K[i][j]\n",
        "\n",
        "        # Check if the graph is connected\n",
        "        n_components, labels = connected_components(adjacency_matrix)\n",
        "        if np.max(labels) == 0:\n",
        "            break  # Graph is connected, exit the loop\n",
        "        print(q)\n",
        "        print(np.max(labels))\n",
        "        print(\"need_increase\")\n",
        "        # Increase q and reset adjacency_matrix\n",
        "        q += increment\n",
        "        adjacency_matrix = np.zeros((num_points, num_points))\n",
        "\n",
        "    return adjacency_matrix\n",
        "```\n",
        "The results of the evaluation were not that good:\n",
        "```\n",
        "Cosine simalirity along with 2000 Mutual Nearest Neighbours with L symmetric Average F1 Score: 0.3610305973806317\n",
        "Weighted Average Recall Score: 0.47286184210526316\n",
        "Weighted Average Precision Score: 0.29700675805383236\n",
        "\n",
        "Cosine simalirity along with 400 Mutual Nearest Neighbours with L asymmetric cosine_plus_plus 29 component\n",
        "Precision : 0.4621895624527028\n",
        "Weighted Average Recall Score: 0.5748355263157895\n",
        "Average F1 Score: 0.5153072434351277\n",
        "\n",
        "Cosine Similarity with 5 Nearest Neighbours with L a cosine_plus_plus_plus\n",
        "Average F1 Score: 0.4424688804852393\n",
        "Weighted Average Recall Score: 0.18530701754385964\n",
        "precision: 0.3791890925050078\n",
        "\n",
        "cosine_plus_plus with pca 605\n",
        "Average F1 Score: 0.46028396770241153\n",
        "Weighted Average Recall Score: 0.5652412280701754\n",
        "precision: 0.39053510548528986\n",
        "\n",
        "cosine_plus_plus with pca 605 with test data Average F1 Score: 0.56031938459708\n",
        "Weighted Average Recall Score: 0.3656798245614035\n",
        "precision: 0.5147547788582703\n",
        "\n",
        "```\n",
        "Untill we used the RBF with nearest neighbours, the results increased a lot:\n",
        "```\n",
        "rbf_plus_plus Average F1 Score: 0.7193104198761674\n",
        "Average Recall Score: 0.7855173956104667\n",
        "Precision: 0.6779438542874393\n",
        "\n",
        "\n",
        "rbf_plus_plus with pca 45 component Average F1 Score: 0.7301748607065958\n",
        "Average Recall Score: 0.7877742818291703\n",
        "Precision: 0.6999127158336077\n",
        "```\n",
        "NOTE: At this stage we were using weighted average in the recall and precision.\n",
        "\n",
        "Now we'll discuss some plots representing the results of spectral clustering:\n",
        "\n",
        "1. Different K's  \n",
        "The bigger the K \"number of components\", the better the results\n",
        "![](https://drive.google.com/uc?export=view&id=1Mhk2vwaq6iSBGK2I5aq-LTk9fsNZlASh)\n",
        "\n",
        "2. Dimensions and Evaluation  \n",
        "The bigger the dimensions, the better the results.\n",
        "![](https://drive.google.com/uc?export=view&id=1qUHEgQ-RgKEfFM6YVDWl0pWuBiaa1Bgo)\n",
        "\n",
        "3. Averaging vs PCA\n",
        "![](https://drive.google.com/uc?export=view&id=1609qkOnxXRU-pmsy1wI-3v63IHD_4HS5)\n",
        "Surprisingly, averaging was better on average, although it was less stable.\n",
        "\n",
        "4. Best Parameters for Spectral Clustering\n",
        "a.  Gamma  \n",
        "![](https://drive.google.com/uc?export=view&id=1MbCWEJV_TOq7mohP6hPm4YmRlUhjPJ5D)\n",
        "\n",
        "In the precision, f1, and conditional entropy the .1 was the best.\n",
        "So we could say that gamma = .1 is the best one.\n",
        "![](https://drive.google.com/file/d/1k19zb6vFQB7s7HdZJiwqj8SJNEIQol_K/view?usp=sharing)\n",
        "b. Nearest n in after RBF  \n",
        "![](https://drive.google.com/uc?export=view&id=1k19zb6vFQB7s7HdZJiwqj8SJNEIQol_K)\n",
        "Best were 400.\n",
        "\n",
        "c. The top Five tuples in the training dataset   \n",
        "![](https://drive.google.com/uc?export=view&id=1_h7Z8qRMX7Xic8g4WL2mefxmPpmcQzCU)\n",
        "Best parameters in the training data were gamma = .1, and nearest neighbours was 400. Although there was higher recalls, but they had very low f1's and precisions and higher conditional entropy. And averaging was better than PCA Let's try these parameters on test data and look at the results.  \n",
        "\n",
        "We optained these resuts:\n",
        "\n",
        "0.1, 400, 0.21655701754385967, 0.7864983095760237, 0.23657642316391342, 2.3411630133950525\n",
        "\n",
        "first number is precision, seond is recall, third is f1, fourth is entropy.\n",
        "\n",
        "â€‹\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
