{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1abqlCbsts_l-2cjm0D_8Z90dq0MPkv0q",
      "authorship_tag": "ABX9TyM9c0d3LhM7oU/xHz1OuCjw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karimfathy054/Clustering/blob/main/Third_clustering_algorithm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Note:\n",
        "\n",
        "the code is tested on small data from sheets it needs further testing"
      ],
      "metadata": {
        "id": "704jKTgwlWvV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading dataset"
      ],
      "metadata": {
        "id": "Hy7OlECaM594"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/MyDrive/activities.zip -d /home"
      ],
      "metadata": {
        "id": "YSNKdaLOOGVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "WcCJemSiM3Np"
      },
      "outputs": [],
      "source": [
        "dataset_path = \"/home/data\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from natsort import natsorted\n",
        "from sklearn.decomposition import PCA"
      ],
      "metadata": {
        "id": "FXP-ZE_hOYK9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(data_path,reduction_method=\"mean\"):\n",
        "  training_data=[]\n",
        "  training_labels=[]\n",
        "  evaluation_data=[]\n",
        "  evaluation_labels=[]\n",
        "  for i,activity in enumerate(natsorted(os.listdir(data_path))):\n",
        "    activity_path = os.path.join(data_path,activity)\n",
        "    for subject in natsorted(os.listdir(activity_path)):\n",
        "      subject_path = os.path.join(activity_path,subject)\n",
        "      data = []\n",
        "      for j,segment in enumerate(natsorted(os.listdir(subject_path))):\n",
        "        segment_path = os.path.join(subject_path,segment)\n",
        "        data_point = pd.read_csv(segment_path,header=None).values\n",
        "        if(reduction_method == \"mean\"):\n",
        "          data.append(np.mean(data_point,axis=0))\n",
        "        elif(reduction_method == \"pca\"):\n",
        "          data.append(data_point.flatten())\n",
        "      if(reduction_method == \"pca\"):\n",
        "        data = np.vstack(data)\n",
        "        data = PCA(n_components=45).fit_transform(data)\n",
        "        data = data.tolist()\n",
        "      training_data += data[:48]\n",
        "      training_labels += [i]*48\n",
        "      evaluation_data += data[48:]\n",
        "      evaluation_labels += [i]*12\n",
        "  return training_data,training_labels,evaluation_data,evaluation_labels"
      ],
      "metadata": {
        "id": "Qv_G-xVyOUHd"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_data,training_labels,evaluation_data,evaluation_labels = load_data(dataset_path)"
      ],
      "metadata": {
        "id": "4_o8tmPoTvM7"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_data,training_labels,evaluation_data,evaluation_labels = load_data(dataset_path,reduction_method=\"pca\")"
      ],
      "metadata": {
        "id": "fKN3kqCEQTKq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# trying sklearn's algorithm"
      ],
      "metadata": {
        "id": "_tQSXLXpSDwO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.metrics.cluster import adjusted_rand_score\n",
        "from sklearn.metrics.cluster import rand_score\n",
        "from sklearn.metrics.cluster import homogeneity_completeness_v_measure"
      ],
      "metadata": {
        "id": "MVIPASDPSI6J"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hc = AgglomerativeClustering(n_clusters = 19,linkage=\"ward\")\n",
        "predictions = hc.fit_predict(training_data)\n",
        "print(rand_score(training_labels,predictions))\n",
        "print(adjusted_rand_score(training_labels,predictions))\n",
        "purity , recall , v_measure = homogeneity_completeness_v_measure(training_labels,predictions)\n",
        "print(purity)\n",
        "print(recall)\n",
        "print(v_measure)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1xIlB9dSmvF",
        "outputId": "5a5fa89c-007e-44f4-9a4b-272a13cec200"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8807571801762804\n",
            "0.2340834704241038\n",
            "0.5137782422513342\n",
            "0.6085773063531851\n",
            "0.5571742022765497\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "02YyXna7X4MU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dbsc = DBSCAN(eps=2,min_samples=5)\n",
        "predictions = dbsc.fit_predict(training_data)\n",
        "adjusted_rand_score(training_labels,predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Dh2BhmHUgBT",
        "outputId": "b8c0d0ea-d30f-4125-be64-0931427c21f0"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.21294252791092105"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUej1KDAVTpd",
        "outputId": "edb69b41-b069-47ba-ad92-22cb27be114f"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   0,    1,    2, ..., 6247, 6248, 6249])"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Expectation Maximization Clustering Algorithm"
      ],
      "metadata": {
        "id": "BjcFNXORY2kz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "expectation maximization is classified as _soft assignment clustering_ algorithm that differs _hard assignment clustering algorithms_ like **k-means**:\n",
        "\n",
        "a data sample can be in multiple clusters at the same time with different probabilities\n",
        "\n"
      ],
      "metadata": {
        "id": "WDzD4wl3ZloR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import multivariate_normal"
      ],
      "metadata": {
        "id": "sww-JVDrcdnq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def expectationMaximization(data,k,max_itr=10000,e=1e-6):\n",
        "    n_samples , n_features = data.shape;\n",
        "\n",
        "    # choose random k from data to act as our random initial means\n",
        "    means = np.array(data[np.random.choice(n_samples,k,replace=False)],dtype=float)\n",
        "\n",
        "    prior_probs = np.ones(k)/k #initial prior probabilities for each cluster\n",
        "    covariances = np.array([np.eye(n_features) for i in range(k)]) # using diagonal covariance matrix for simplicity\n",
        "    weights = np.zeros((n_samples,k))\n",
        "    for itr in range(max_itr):\n",
        "        # Expectation\n",
        "        # calculating probability that each sample is in a given class\n",
        "        for i in range(n_samples):\n",
        "            for j in range(k):\n",
        "                weights[i,j] = prior_probs[j]*multivariate_normal(mean = means[j],cov = covariances[j],allow_singular=True).pdf(data[i])\n",
        "            weights[i] = weights[i]/np.sum(weights[i])+1e-6\n",
        "\n",
        "\n",
        "        # Maximization step\n",
        "        old_means = means.copy()\n",
        "        for i in range(k):\n",
        "            means[i] = np.dot(data.T,weights[:,i])/np.sum(weights[:,i]) # calculating new mean for each class based on the probability for all the points to be belonging to it\n",
        "            covariances[i] = np.sum(\n",
        "                np.array([weights[j,i]*np.outer(data[j]-means[i],data[j]-means[i]) for j in range(n_samples)]),axis=0\n",
        "                )/np.sum(weights[:,i]) # calculating new covariance matrices using the new calculated means\n",
        "            prior_probs[i] = np.sum(weights[:,i])/n_samples\n",
        "\n",
        "        if np.all(means-old_means)<e:\n",
        "            break\n",
        "    predictions = np.argmax(weights,axis=1)\n",
        "    return predictions,means,covariances;\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "W7AnrayhbwAJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions,means,covariances = expectationMaximization(np.vstack(training_data),k=19)\n",
        "predictions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJxsL8YCdxS9",
        "outputId": "42baa044-abdc-4d85-e777-d0f60507cdb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([9, 9, 9, ..., 3, 3, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import rand_score\n",
        "rand_score(training_labels,predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-xQVpLoigCU",
        "outputId": "ba39dd1e-3d15-4f24-98b6-1c0b39275f6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9135442218895422"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hierarchical clustering\n",
        "\n",
        "not running yet"
      ],
      "metadata": {
        "id": "0YKr5rS_hgaT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import pairwise_distances"
      ],
      "metadata": {
        "id": "LBdFEhcFiCHi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cluster_distance(cluster1, cluster2, distance_between_clusters=\"single_link\"):\n",
        "    distance = 0.0;\n",
        "    if(distance_between_clusters == \"single_link\"):\n",
        "        distance = np.min(pairwise_distances(cluster1,cluster2))\n",
        "    elif(distance_between_clusters == \"complete_link\"):\n",
        "        distance = np.max(pairwise_distances(cluster1,cluster2))\n",
        "    elif(distance_between_clusters == \"group_average\"):\n",
        "        distance = np.mean(pairwise_distances(cluster1,cluster2))\n",
        "    elif(distance_between_clusters == \"mean_distance\"):\n",
        "        mean1= np.mean(np.vstack(cluster1),axis=0)\n",
        "        mean2= np.mean(np.vstack(cluster2),axis=0)\n",
        "        distance = np.linalg.norm(mean1-mean2)\n",
        "    elif(distance_between_clusters == \"wards\"):\n",
        "        mean1= np.mean(np.vstack(cluster1),axis=0)\n",
        "        mean2= np.mean(np.vstack(cluster2),axis=0)\n",
        "        distance = (len(cluster1)*len(cluster2)/len(cluster1)+len(cluster2))*np.linalg.norm(mean1-mean2)**2\n",
        "    return distance"
      ],
      "metadata": {
        "id": "K89kVW6chcTi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def hierarchical_clustering(data,k,distance_between_clusters = \"single_link\"):\n",
        "    n_samples = data.shape[0]\n",
        "    n_clusters = n_samples\n",
        "    distances = pairwise_distances(data)\n",
        "    clusters=[]\n",
        "    for point in data:\n",
        "        clusters.append([point])\n",
        "    for i in range(n_samples):\n",
        "        distances[i,i] = np.inf\n",
        "    while n_clusters > k:\n",
        "        min_indx1,min_indx2 = np.unravel_index(np.argmin(distances),distances.shape)\n",
        "        culster1 = clusters[min_indx1]\n",
        "        culster2 = clusters[min_indx2]\n",
        "        Cij = culster1 + culster2\n",
        "\n",
        "        for i in range(n_clusters):\n",
        "            if(i == min_indx1 or i == min_indx2):\n",
        "                continue\n",
        "            dCiCr = cluster_distance(clusters[min_indx1],clusters[i],distance_between_clusters)\n",
        "            dCjCr = cluster_distance(clusters[min_indx2],clusters[i],distance_between_clusters)\n",
        "            dCiCj = cluster_distance(clusters[min_indx1],clusters[min_indx2],distance_between_clusters)\n",
        "            alpha_i = 0.0\n",
        "            alpha_j = 0.0\n",
        "            beta = 0.0\n",
        "            gamma = 0.0\n",
        "            if(distance_between_clusters == \"single_link\"):\n",
        "                alpha_i = 0.5\n",
        "                alpha_j = 0.5\n",
        "                beta = 0\n",
        "                gamma = -0.5\n",
        "            elif(distance_between_clusters == \"complete_link\"):\n",
        "                alpha_i = 0.5\n",
        "                alpha_j = 0.5\n",
        "                beta = 0\n",
        "                gamma = 0.5\n",
        "            elif(distance_between_clusters == \"group_average\"):\n",
        "                alpha_i = len(clusters[min_indx1])/(len(clusters[min_indx1])+len(clusters[min_indx2]))\n",
        "                alpha_j = len(clusters[min_indx2])/(len(clusters[min_indx1])+len(clusters[min_indx2]))\n",
        "                beta = 0\n",
        "                gamma = 0\n",
        "            elif(distance_between_clusters == \"mean_distance\"):\n",
        "                alpha_i = len(clusters[min_indx1])/(len(clusters[min_indx1])+len(clusters[min_indx2]))\n",
        "                alpha_j = len(clusters[min_indx2])/(len(clusters[min_indx1])+len(clusters[min_indx2]))\n",
        "                beta = -len(clusters[min_indx1])*len(clusters[min_indx2])/(len(clusters[min_indx1])+len(clusters[min_indx2]))**2\n",
        "                gamma = 0\n",
        "            elif(distance_between_clusters == \"wards\"):\n",
        "                alpha_i = len(clusters[min_indx1])+len(clusters[i])/len(clusters[min_indx1])+len(clusters[min_indx2])+len(clusters[i])\n",
        "                alpha_j = len(clusters[min_indx2])+len(clusters[i])/len(clusters[min_indx1])+len(clusters[min_indx2])+len(clusters[i])\n",
        "                beta  = -len(clusters[i])/len(clusters[min_indx1])+len(clusters[min_indx2])+len(clusters[i])\n",
        "                gamma = 0\n",
        "            distances[min_indx1,i]=distances[i,min_indx1] = alpha_i*dCiCr + alpha_j*dCjCr + beta*dCiCj + gamma*np.abs(dCiCr-dCjCr)\n",
        "        clusters[min_indx1] = Cij\n",
        "        clusters.pop(min_indx2)\n",
        "        distances = np.delete(distances,min_indx2,axis=0)\n",
        "        distances = np.delete(distances,min_indx2,axis=1)\n",
        "        n_clusters -= 1\n",
        "    return clusters,distances"
      ],
      "metadata": {
        "id": "gX11dLybhqCd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}