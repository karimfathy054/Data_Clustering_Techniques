{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1abqlCbsts_l-2cjm0D_8Z90dq0MPkv0q",
      "authorship_tag": "ABX9TyOa8yREokwu6jWJps+R6HO/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karimfathy054/Clustering/blob/main/Third_clustering_algorithm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading dataset"
      ],
      "metadata": {
        "id": "Hy7OlECaM594"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/MyDrive/activities.zip -d /home"
      ],
      "metadata": {
        "id": "YSNKdaLOOGVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WcCJemSiM3Np"
      },
      "outputs": [],
      "source": [
        "dataset_path = \"/home/data\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from natsort import natsorted\n",
        "from sklearn.decomposition import PCA"
      ],
      "metadata": {
        "id": "FXP-ZE_hOYK9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(data_path,reduction_method=\"mean\"):\n",
        "  training_data=[]\n",
        "  training_labels=[]\n",
        "  evaluation_data=[]\n",
        "  evaluation_labels=[]\n",
        "  for i,activity in enumerate(natsorted(os.listdir(data_path))):\n",
        "    activity_path = os.path.join(data_path,activity)\n",
        "    for subject in natsorted(os.listdir(activity_path)):\n",
        "      subject_path = os.path.join(activity_path,subject)\n",
        "      data = []\n",
        "      for j,segment in enumerate(natsorted(os.listdir(subject_path))):\n",
        "        segment_path = os.path.join(subject_path,segment)\n",
        "        data_point = pd.read_csv(segment_path,header=None).values\n",
        "        if(reduction_method == \"mean\"):\n",
        "          data.append(np.mean(data_point,axis=0))\n",
        "        elif(reduction_method == \"pca\"):\n",
        "          data.append(data_point.flatten())\n",
        "      if(reduction_method == \"pca\"):\n",
        "        pca = PCA(n_components=45)\n",
        "        x = np.vstack(data[:48])\n",
        "        x = pca.fit_transform(x)\n",
        "        x = x.tolist()\n",
        "        training_data += x\n",
        "        y = np.vstack(data[48:])\n",
        "        y = pca.transform(y)\n",
        "        y=y.tolist()\n",
        "        evaluation_data += y\n",
        "      else:\n",
        "        training_data += data[:48]\n",
        "        evaluation_data += data[48:]\n",
        "      training_labels += [i]*48\n",
        "      evaluation_labels += [i]*12\n",
        "  return training_data,training_labels,evaluation_data,evaluation_labels"
      ],
      "metadata": {
        "id": "Qv_G-xVyOUHd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "# Using the mean solutions for representing a segment"
      ],
      "metadata": {
        "id": "rbA0pJE6T6q1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_data,training_labels,evaluation_data,evaluation_labels = load_data(dataset_path)\n",
        "training_data = np.vstack(training_data)\n",
        "evaluation_data = np.vstack(evaluation_data)"
      ],
      "metadata": {
        "id": "4_o8tmPoTvM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Expectation Maximization Clustering Algorithm"
      ],
      "metadata": {
        "id": "BjcFNXORY2kz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "expectation maximization is _soft assignment clustering_ algorithm .\n",
        "\n",
        "Unlike **k-means** which is a _hard assignment clustering algorithms where each point can belong to only one cluster_, each point has a probability of belonging to each cluster .\n",
        "\n",
        "If we assume that each cluster in our dataset is characterized by a multivariate normal distribution:\n",
        "\n",
        "$$\n",
        "f_i(X)=f(X|μ_i,∑_i) = \\frac{1}{(2π)^\\frac{d}{2} |∑_i|_\\frac{1}{2}}\n",
        "exp\\left \\{\\frac{(x-μ_i)^T ∑_i^{-1} (x-μ_i)}{2} \\right \\}\n",
        "$$\n",
        "\n",
        "for a k cluster dataset its normal distribution is defined as the sum of all k clusters poterior probabilities\n",
        "\n",
        "for our each point **D_i** in dataset **D** we need to find the correct cluster **C** that gives us the maximum conditional probability of   \n",
        "$$\n",
        "  P(D_i|C)\n",
        "$$\n",
        "Altough, we don't know the correct means and covariances of our data the algorithm starts with and initial guess and goes through two steps:\n",
        "1. **expectation step** where it uses the estimate for the parameters (clusters means,clusters covariances) to compute the posterior probablity for each class at each data point.\"the contribution probability of each data point to each assumed cluster (weights)\"\n",
        "2. **Maximization step** where it assumes that all the weights computed previously are the correct ones and refines the parameters accordingly by recomputing the mean and covariance of each cluster\n",
        "\n",
        "The algorithm keeps runnin till it reaches the maximum number of iterations or the parameters start to converge\n",
        "\n",
        "Notes:\n",
        "the algorithm uses initial means by selecting points from the dataset and initializes clusters covariances with diagonal identity matrices for simplicity\n",
        "\n"
      ],
      "metadata": {
        "id": "WDzD4wl3ZloR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "using available liberaries to calculate the likelihood function\n",
        "[scipy.stats.multivariate_normal](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.multivariate_normal.html#scipy-stats-multivariate-normal)"
      ],
      "metadata": {
        "id": "GphKJi-3jJu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import multivariate_normal"
      ],
      "metadata": {
        "id": "sww-JVDrcdnq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def expectationMaximization(data,k,max_itr=1000,e=1e-6):\n",
        "    n_samples , n_features = data.shape;\n",
        "\n",
        "    # choose random k from data to act as our random initial means\n",
        "    means = np.array(data[np.random.choice(n_samples,k,replace=False)],dtype=float)\n",
        "\n",
        "    prior_probs = np.ones(k)/k #initial prior probabilities for each cluster\n",
        "    covariances = np.array([np.eye(n_features) for i in range(k)]) # using diagonal covariance matrix for simplicity\n",
        "    weights = np.zeros((n_samples,k))\n",
        "    for itr in range(max_itr):\n",
        "        # Expectation\n",
        "        # calculating probability that each sample is in a given class\n",
        "        for j in range(k):\n",
        "          mvn = multivariate_normal(mean = means[j],cov = covariances[j])\n",
        "          for i in range(n_samples):\n",
        "            weights[i,j] = prior_probs[j]*mvn.pdf(data[i])\n",
        "        for i in range(n_samples):\n",
        "          weights[i] = weights[i]/np.sum(weights[i])+1e-6\n",
        "\n",
        "\n",
        "        # Maximization step\n",
        "        old_means = means.copy()\n",
        "        for i in range(k):\n",
        "            means[i] = np.dot(data.T,weights[:,i])/np.sum(weights[:,i]) # calculating new mean for each class based on the probability for all the points to be belonging to it\n",
        "            covariances[i] = np.sum(\n",
        "                np.array([weights[j,i]*np.outer(data[j]-means[i],data[j]-means[i]) for j in range(n_samples)]),axis=0\n",
        "                )/np.sum(weights[:,i]) # calculating new covariance matrices using the new calculated means\n",
        "            prior_probs[i] = np.sum(weights[:,i])/n_samples\n",
        "\n",
        "        if np.allclose(means,old_means,atol=e):\n",
        "            break\n",
        "    predictions = np.argmax(weights,axis=1)\n",
        "    return predictions,means,covariances;"
      ],
      "metadata": {
        "id": "W7AnrayhbwAJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "Cy-x8QUWPx_j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## using training data\n",
        "\n",
        "we will use use training data in EM-Algorithm for (k=19),as we know that the correct number of clusters should be 19, to find the best centroids {means and covariances} for the data.\n",
        "\n",
        "we run the algorithm mutliple times to get the best scores posiible due to the effect, that maybe not as significant, of the randomized initial means\n",
        "\n",
        "we will then use those parameters to classify our evaluation dataset then measure:\n",
        "1. accuarcy\n",
        "2. recall\n",
        "3. F-measure\n",
        "4. conditional entropy\n"
      ],
      "metadata": {
        "id": "s0E1p_e_b0VD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "using sklearn's evaluations measures that implement the needed measure with different names\n",
        "\n",
        "[homogeneity completness and V-score from sklearn.metrics](https://scikit-learn.org/stable/modules/clustering.html#homogeneity-completeness-and-v-measure)\n",
        "\n",
        "**homogeneity** score is equivalent to our percision/purity score, where it measures how much a cluster contains only members of a single class\n",
        "\n",
        "**completeness** score is equivalent to recall measure\n",
        "\n",
        "**V-score** is equivalent to F-measure"
      ],
      "metadata": {
        "id": "ssgUEpOVYaau"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import homogeneity_completeness_v_measure"
      ],
      "metadata": {
        "id": "Sj6DNLfEgtiH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def conditional_entropy(true_labels,predictions):\n",
        "  n_samples = len(true_labels)\n",
        "  true_labels = np.array(true_labels)\n",
        "  predictions = np.array(predictions)\n",
        "  clusters={}\n",
        "  cluster_ids = np.unique(predictions)\n",
        "\n",
        "  for id in cluster_ids:\n",
        "    cluster = true_labels[predictions == id]\n",
        "    clusters[id]=cluster\n",
        "\n",
        "  true_clusters_ids = np.unique(true_labels)\n",
        "  entropy = 0.0\n",
        "  for i,cluster in enumerate(clusters.values()):\n",
        "    cluster = np.array(cluster)\n",
        "    cluster_entropy = 0.0\n",
        "    for label in  true_clusters_ids:\n",
        "      count = np.where(cluster == label)[0].size\n",
        "      if(count!=0):\n",
        "        cluster_entropy -= (count/cluster.size)*np.log(count/cluster.size)\n",
        "    entropy += (cluster.size/n_samples) * cluster_entropy\n",
        "  return entropy\n"
      ],
      "metadata": {
        "id": "_zSxmAsaIyf4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def percision_recall_f1_score(true_labels,predictions):\n",
        "  n_samples = len(true_labels)\n",
        "  true_labels = np.array(true_labels)\n",
        "  t_labels,true_counts = np.unique(true_labels,return_counts=True)\n",
        "  predictions = np.array(predictions)\n",
        "  clusters={}\n",
        "  cluster_ids = np.unique(predictions)\n",
        "  for id in cluster_ids:\n",
        "    cluster = true_labels[predictions == id]\n",
        "    clusters[id]=cluster\n",
        "  percision = 0.0\n",
        "  recall = 0.0\n",
        "  f1_scores = []\n",
        "  flag=True\n",
        "  i=0\n",
        "\n",
        "  for cluster in clusters.values():\n",
        "    elements_classes,counts = np.unique(cluster,return_counts=True)\n",
        "    class_purity = (np.max(counts)/cluster.size)\n",
        "    percision+=(np.max(counts)/n_samples)\n",
        "    max_element = elements_classes[np.argmax(counts)]\n",
        "    class_recall  = np.max(counts)/true_counts[t_labels==max_element][0]\n",
        "    recall+=class_recall*(cluster.size/n_samples)\n",
        "    Fi = (2*class_purity*class_recall)/(class_purity+class_recall)\n",
        "    f1_scores.append(Fi)\n",
        "  return percision,recall,np.mean(f1_scores)"
      ],
      "metadata": {
        "id": "Jhm8P-Yoh46e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions1,means1,covariances1 = expectationMaximization(training_data,19)\n",
        "precision1 , recall1 , f1_score1 = percision_recall_f1_score(training_labels,predictions1)\n",
        "homogeneity1 , completeness1 , v_measure1 = homogeneity_completeness_v_measure(training_labels,predictions1)"
      ],
      "metadata": {
        "id": "9w_Pp35bpnpA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conditional_entropy1 = conditional_entropy(training_labels,predictions1)"
      ],
      "metadata": {
        "id": "g9PXQy6SJBl_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(precision1)\n",
        "print(recall1)\n",
        "print(f1_score1)\n",
        "print(conditional_entropy1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2xZJsWRHP62S",
        "outputId": "509371de-7b96-4c21-fcd4-eeec3d8e8a80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.46367872807017535\n",
            "0.5647932799936037\n",
            "0.44101290226516715\n",
            "1.2898399669025866\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions2,means2,covariances2 = expectationMaximization(np.vstack(training_data),19)\n",
        "precision2 , recall2 , f1_score2 = percision_recall_f1_score(training_labels,predictions2)\n",
        "homogeneity2 , completeness2 , v_measure2 = homogeneity_completeness_v_measure(training_labels,predictions2)"
      ],
      "metadata": {
        "id": "SjTp9k0jm5AL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conditional_entropy2 = conditional_entropy(training_labels,predictions2)"
      ],
      "metadata": {
        "id": "CB8II5EiJNpV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(precision2)\n",
        "print(recall2)\n",
        "print(f1_score2)\n",
        "print(conditional_entropy2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rRa-fP-fQEia",
        "outputId": "7d8645de-205f-4d7d-c880-0b110306c87b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.493969298245614\n",
            "0.648032740542763\n",
            "0.47541760559242396\n",
            "1.2539087924774655\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions3,means3,covariances3 = expectationMaximization(np.vstack(training_data),19)\n",
        "precision3 , recall3 , f1_score3 = percision_recall_f1_score(training_labels,predictions3)\n",
        "homogeneity3 , completeness3 , v_measure3 = homogeneity_completeness_v_measure(training_labels,predictions3)"
      ],
      "metadata": {
        "id": "zRq29Gt9m5hZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conditional_entropy3 = conditional_entropy(training_labels,predictions3)"
      ],
      "metadata": {
        "id": "wbkoq2BuJP91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(precision3)\n",
        "print(recall3)\n",
        "print(f1_score3)\n",
        "print(conditional_entropy3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tky9glUYQLPJ",
        "outputId": "73558555-c090-496e-a728-9885ad28bbc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5223410087719298\n",
            "0.6451009114583334\n",
            "0.4987516864225105\n",
            "1.2079850325993435\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## using the centroids from the clustering with best scores we will classify the points in evaluation set using gaussian bayes classifier and the paramters of the best clustering found"
      ],
      "metadata": {
        "id": "4inTra_mnveW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def GaussianBayesClassifier(data,clusters,means,covariances):\n",
        "  prior_probs = np.zeros(len(means))\n",
        "  clusters = np.array(clusters)\n",
        "  clusters,sizes = np.unique(predictions1,return_counts=True)\n",
        "  for i,cluster,size in zip(range(clusters.size),clusters,sizes):\n",
        "    prob = (size/clusters.size)\n",
        "    prior_probs[i] = prob\n",
        "\n",
        "  n_points = data.shape[0]\n",
        "  predictions = np.full(n_points,-1)\n",
        "  for i,point in enumerate(data):\n",
        "    probs = [(np.log(prob) + multivariate_normal(mean=m,cov=sigma).logpdf(point)) for prob,m,sigma in zip(prior_probs,means1,covariances1)] # using log of probability to avoid underflow of small float numbers\n",
        "    predictions[i]=np.argmax(probs)\n",
        "  return predictions"
      ],
      "metadata": {
        "id": "ae50qRoxPmEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "1Rryv-ZhiwmI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation_predictions = GaussianBayesClassifier(evaluation_data,predictions3,means3,covariances3)"
      ],
      "metadata": {
        "id": "0r9TMiP6Pmnj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_percision,eval_recall,eval_f1_score = percision_recall_f1_score(evaluation_labels,evaluation_predictions)\n",
        "eval_homogeneity , eval_completeness , eval_v_measure = homogeneity_completeness_v_measure(evaluation_labels,evaluation_predictions)\n",
        "eval_entropy = conditional_entropy(evaluation_labels,evaluation_predictions)"
      ],
      "metadata": {
        "id": "FcxdxDqFUtEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(eval_percision)\n",
        "print(eval_recall)\n",
        "print(eval_f1_score)\n",
        "print(eval_entropy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "myLNAyLSVF8S",
        "outputId": "c2484d3e-fe9b-4548-9a10-1ce91ff18346"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.4473684210526316\n",
            "0.5941154970760234\n",
            "0.41873502186131606\n",
            "1.3102833341648512\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Qbj3ZMRSkg1Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Running the experiment with PCA dim-reduction"
      ],
      "metadata": {
        "id": "XKUMENCJVMXG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca_training_data,pca_training_labels,pca_evaluation_data,pca_evaluation_labels = load_data(dataset_path,reduction_method=\"pca\")\n",
        "pca_training_data = np.vstack(training_data)\n",
        "pca_evaluation_data = np.vstack(evaluation_data)"
      ],
      "metadata": {
        "id": "t5ZzQxDuVUiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "m5GnLyYgj7IM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca_predictions1,pca_means1,pca_variances1 = expectationMaximization(pca_training_data,19)"
      ],
      "metadata": {
        "id": "tBLUxVwsVaQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pca_percision1,pca_recall1,pca_f1_score1 = percision_recall_f1_score(pca_training_labels,pca_predictions1)\n",
        "pca_homogeneity1,pca_completeness1,pca_V_measure1 = homogeneity_completeness_v_measure(pca_training_labels,pca_predictions1)\n",
        "\n",
        "pca_entropy1 = conditional_entropy(pca_training_labels,pca_predictions1)"
      ],
      "metadata": {
        "id": "uvVH1XkJV5tr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(pca_percision1)\n",
        "print(pca_recall1)\n",
        "print(pca_f1_score1)\n",
        "print(pca_entropy1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HC5rAiStWcDs",
        "outputId": "3f69afc6-34a5-49df-a244-b687eddb447b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.4568256578947369\n",
            "0.5057794225146198\n",
            "0.456225831265302\n",
            "1.2893871644955621\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "EHXNbaaJWrrU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca_predictions2,pca_means2,pca_variances2 = expectationMaximization(pca_training_data,19)"
      ],
      "metadata": {
        "id": "YeyRhBzPWRR4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pca_percision2,pca_recall2,pca_f1_score2 = percision_recall_f1_score(pca_training_labels,pca_predictions2)\n",
        "pca_homogeneity2,pca_completeness2,pca_V_measure2 = homogeneity_completeness_v_measure(pca_training_labels,pca_predictions2)\n",
        "\n",
        "pca_entropy2 = conditional_entropy(pca_training_labels,pca_predictions2)"
      ],
      "metadata": {
        "id": "pURHiWNKWVLm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(pca_percision2)\n",
        "print(pca_recall2)\n",
        "print(pca_f1_score2)\n",
        "print(pca_entropy2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tGpEFBGLWihw",
        "outputId": "a224b713-de00-4cb7-9440-c58d64d3a3aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.4931469298245613\n",
            "0.6262967293722587\n",
            "0.4730916985236783\n",
            "1.2552830449567363\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "CS36EOrmWtw_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca_predictions3,pca_means3,pca_variances3 = expectationMaximization(pca_training_data,19)"
      ],
      "metadata": {
        "id": "6FcOTX8rWvSd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pca_percision3,pca_recall3,pca_f1_score3 = percision_recall_f1_score(pca_training_labels,pca_predictions3)\n",
        "pca_homogeneity3,pca_completeness3,pca_V_measure3 = homogeneity_completeness_v_measure(pca_training_labels,pca_predictions3)\n",
        "\n",
        "pca_entropy3 = conditional_entropy(pca_training_labels,pca_predictions3)"
      ],
      "metadata": {
        "id": "vl1j2oOZWyTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(pca_percision3)\n",
        "print(pca_recall3)\n",
        "print(pca_f1_score3)\n",
        "print(pca_entropy3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDxLTKKTW2rn",
        "outputId": "a8d388b1-324e-4197-d353-ce0e84e7b94b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.4668311403508772\n",
            "0.6348662794682017\n",
            "0.436869306876516\n",
            "1.3004858495311857\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "H8J-hvrpXP00"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## using evaluation set in pca"
      ],
      "metadata": {
        "id": "Rm7Cnz6iX2mQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca_evaluation_predictions = GaussianBayesClassifier(evaluation_data,pca_percision2,pca_means2,pca_variances2)"
      ],
      "metadata": {
        "id": "ubMYz0a5XRzW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pca_eval_percision,pca_eval_recall,pca_eval_f1_score = percision_recall_f1_score(evaluation_labels,pca_evaluation_predictions)\n",
        "pca_eval_percision,pca_eval_recall,pca_eval_f1_score = homogeneity_completeness_v_measure(evaluation_labels,pca_evaluation_predictions)\n",
        "\n",
        "pca_eval_entropy = conditional_entropy(evaluation_labels,pca_evaluation_predictions)"
      ],
      "metadata": {
        "id": "YffNA_bKXiC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(pca_eval_percision)\n",
        "print(pca_eval_recall)\n",
        "print(pca_eval_f1_score)\n",
        "print(pca_eval_entropy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yStb952OXqui",
        "outputId": "a3ddf9e0-eaf8-4849-c677-54901e61655a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5549972869413013\n",
            "0.5915680260979066\n",
            "0.5726994280949993\n",
            "1.3102833341648512\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Both PCA and Segment Mean solutions have nearly similar accuracies excpet for a slight improvment in the PCA solution\n",
        "on evaluation set only\n",
        "\n"
      ],
      "metadata": {
        "id": "IFN-wwyFmq_H"
      }
    }
  ]
}
